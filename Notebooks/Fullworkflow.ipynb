{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit recognition using a simple neural network\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_input_img(X, y):\n",
    "    # Create a figure with 5 subplots side by side\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "    \n",
    "    # Loop over the first 5 images\n",
    "    for i in range(5):\n",
    "        # Display the image in grayscale\n",
    "        axes[i].imshow(X[i], cmap='gray')\n",
    "        # Set the title of the subplot with the corresponding label\n",
    "        axes[i].set_title(y[i])\n",
    "        # Turn off the axes for better visualization\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Display the figure\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the first 5 images of the training set\n",
    "plot_input_img(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the training data by converting it to float32 and scaling the pixel values to the range [0, 1]\n",
    "X_train = X_train.astype(np.float32) / 255\n",
    "\n",
    "# Normalize the test data by converting it to float32 and scaling the pixel values to the range [0, 1]\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "\n",
    "#  reshape / expand the dimensions of images to 28x28x1\n",
    "# Reshape the training data to add an extra dimension for the channel (28x28x1)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "\n",
    "# Reshape the test data to add an extra dimension for the channel (28x28x1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "# Why Add a New Dimension?:\n",
    "\n",
    "# Many deep learning frameworks (like TensorFlow or PyTorch) expect image data to have a channel dimension, even if it's just one channel for grayscale images.\n",
    "# For example, Convolutional Neural Networks (CNNs) typically expect input data in the format (height, width, channels). Adding this extra dimension ensures compatibility with these models.\n",
    "\n",
    "# Convert the training labels to one-hot encoded vectors\n",
    "# This means that each label will be represented as a vector with a length equal to the number of classes (10 in this case)\n",
    "# For example, if the label is 3, the one-hot encoded vector will be [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "# This is done to make it easier to train the neural network\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "\n",
    "# Convert the test labels to one-hot encoded vectors\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pieta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add a 2D convolutional layer with 32 filters, a kernel size of 3x3, ReLU activation, and input shape of 28x28x1\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "# Add a max pooling layer with a pool size of 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add another 2D convolutional layer with 64 filters, a kernel size of 3x3, and ReLU activation\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# Add another max pooling layer with a pool size of 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output of the previous layer to create a 1D feature vector\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a dropout layer with a dropout rate of 0.5 to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a dense (fully connected) layer with 10 units and ReLU activation\n",
    "model.add(Dense(10, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCompiles the model with the specified optimizer, loss function, and evaluation metrics.\\n\\nThis function configures the model for training by specifying the optimizer to use, \\nthe loss function to minimize, and the metrics to evaluate during training and testing.\\n\\nParameters:\\n    optimizer (str): The optimizer to use for training. In this case, 'adam' is used.\\n    loss (function): The loss function to minimize. Here, keras.losses.categorical_crossentropy is used.\\n    metrics (list): A list of metrics to evaluate during training and testing. Here, ['accuracy'] is used.\\n\\nReturns:\\n    None\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\"\"\"\n",
    "Compiles the model with the specified optimizer, loss function, and evaluation metrics.\n",
    "\n",
    "This function configures the model for training by specifying the optimizer to use, \n",
    "the loss function to minimize, and the metrics to evaluate during training and testing.\n",
    "\n",
    "Parameters:\n",
    "    optimizer (str): The optimizer to use for training. In this case, 'adam' is used.\n",
    "    loss (function): The loss function to minimize. Here, keras.losses.categorical_crossentropy is used.\n",
    "    metrics (list): A list of metrics to evaluate during training and testing. Here, ['accuracy'] is used.\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#  EarlyStopping callback to stop training if the validation loss does not improve for 3 consecutive epochs\n",
    "# ModelCheckpoint callback to save the model with the best validation loss\n",
    "# ModelCheckpoint callback to save the model with the best validation loss in Keras format\n",
    "\n",
    "# EarlyStopping callback to stop training if the validation accuracy does not improve for 3 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# ModelCheckpoint callback to save only the best weights with the best validation accuracy\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0982 - loss: nan - val_accuracy: 0.0995 - val_loss: nan\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.0982 - loss: nan - val_accuracy: 0.0995 - val_loss: nan\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0991 - loss: nan - val_accuracy: 0.0995 - val_loss: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0978 - loss: nan - val_accuracy: 0.0995 - val_loss: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0967 - loss: nan - val_accuracy: 0.0995 - val_loss: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0991 - loss: nan - val_accuracy: 0.0995 - val_loss: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0993 - loss: nan - val_accuracy: 0.0995 - val_loss: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1003 - loss: nan - val_accuracy: 0.0995 - val_loss: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0996 - loss: nan - val_accuracy: 0.0995 - val_loss: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1001 - loss: nan - val_accuracy: 0.0995 - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "his = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# save the model\n",
    "\n",
    "model.save('digit_recognition_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saved = keras.models.load_model(r'C:\\Users\\pieta\\OneDrive\\Bureau\\GAN\\Notebooks\\digit_recognition_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model accuracy is 0.09799999743700027\n"
     ]
    }
   ],
   "source": [
    "score = model_saved.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"the model accuracy is {score[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
